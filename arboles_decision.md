# üå≥ √Årboles de Decisi√≥n: Gu√≠a Completa

Este documento explica qu√© son los √°rboles de decisi√≥n, c√≥mo se construyen y proporciona ejemplos pr√°cticos.

---

## 1. ¬øQu√© son los √Årboles de Decisi√≥n?

Un **√Årbol de Decisi√≥n** es un modelo de **aprendizaje autom√°tico supervisado** que se utiliza tanto para tareas de **clasificaci√≥n** (predecir una categor√≠a, como "S√≠/No" o "Spam/No Spam") como de **regresi√≥n** (predecir un valor num√©rico, como el precio de una casa).

Visualmente, se asemeja a un diagrama de flujo o un √°rbol invertido. Est√° compuesto por:

* **Nodo Ra√≠z (Root Node):** El punto de partida del √°rbol, que representa a todo el conjunto de datos.
* **Nodos Internos (Internal Nodes):** Representan una "pregunta" o una prueba sobre un atributo (una caracter√≠stica) del dato.
* **Ramas (Branches):** Representan el resultado de la "pregunta" (la respuesta). Cada rama conecta un nodo con otro.
* **Nodos Hoja (Leaf Nodes):** Son los nodos finales del √°rbol. Representan la decisi√≥n final o la predicci√≥n (una etiqueta de clase o un valor).



[Image of a simple decision tree diagram]


La idea principal es **dividir los datos** en subconjuntos cada vez m√°s peque√±os y "puros" (homog√©neos) bas√°ndose en las preguntas de los nodos, hasta que se pueda tomar una decisi√≥n clara en un nodo hoja.

---

## 2. ¬øC√≥mo se construyen?

La construcci√≥n de un √°rbol de decisi√≥n, generalmente usando algoritmos como **ID3**, **C4.5** o **CART**, sigue un proceso llamado **particionamiento recursivo**.

El objetivo es encontrar, en cada paso, la "mejor" pregunta (el mejor atributo) para dividir los datos. ¬øPero qu√© significa "mejor"?

Significa elegir el atributo que mejor separe los datos en grupos que sean lo m√°s **homog√©neos** posible. Para medir esto, se usan principalmente dos m√©tricas:

1.  **Ganancia de Informaci√≥n (Information Gain) (Usada en ID3, C4.5):**
    * Mide la reducci√≥n de la **entrop√≠a**.
    * La *entrop√≠a* es una medida del desorden o la impureza en un conjunto de datos. Un conjunto con 50% de "S√≠" y 50% de "No" tiene entrop√≠a m√°xima (m√°ximo desorden). Un conjunto con 100% de "S√≠" tiene entrop√≠a cero (totalmente puro).
    * El algoritmo elige el atributo que proporciona la **mayor ganancia de informaci√≥n**, es decir, el que m√°s reduce el desorden.

2.  **Impureza de Gini (Gini Impurity) (Usada en CART):**
    * Mide la probabilidad de clasificar incorrectamente un elemento si se eligiera al azar.
    * Un valor de Gini de 0 significa que el nodo es perfectamente puro (todos los elementos son de la misma clase).
    * El algoritmo elige el atributo que resulta en la **menor impureza de Gini** promedio para los subconjuntos resultantes.

### Proceso de Construcci√≥n (Simplificado):

1.  **Empezar en el Nodo Ra√≠z:** Se toma todo el conjunto de datos.
2.  **Encontrar el mejor atributo:** Se calcula la Ganancia de Informaci√≥n (o la Impureza de Gini) para *cada* atributo disponible.
3.  **Dividir (Split):** Se selecciona el atributo con la mejor m√©trica (mayor ganancia o menor impureza) y se convierte en el nodo de decisi√≥n.
4.  **Crear Ramas:** Se crea una rama para cada posible valor de ese atributo.
5.  **Repetir:** Para cada rama, se crea un nuevo subconjunto de datos. Se repiten los pasos 2 al 4 de forma recursiva para cada subconjunto.
6.  **Parar:** El proceso se detiene para una rama cuando:
    * El nodo resultante es **puro** (todos los datos pertenecen a la misma clase).
    * No quedan m√°s atributos para dividir.
    * Se alcanzan condiciones predefinidas (como una profundidad m√°xima del √°rbol o un n√∫mero m√≠nimo de muestras por hoja), esto se hace para evitar el **sobreajuste (overfitting)**.

---

## 3. Ejemplo de Construcci√≥n (Cl√°sico)

Imaginemos que queremos decidir si **"Jugar al Tenis" (S√≠/No)** bas√°ndonos en el clima.

**Datos:**

| Cielo | Temperatura | Humedad | Viento | ¬øJugar? |
| :--- | :--- | :--- | :--- | :--- |
| Soleado| Caluroso | Alta | D√©bil | No |
| Soleado| Caluroso | Alta | Fuerte | No |
| Nublado| Caluroso | Alta | D√©bil | S√≠ |
| Lluvia | Templado | Alta | D√©bil | S√≠ |
| Lluvia | Fr√≠o | Normal | D√©bil | S√≠ |
| Lluvia | Fr√≠o | Normal | Fuerte | No |
| Nublado| Fr√≠o | Normal | Fuerte | S√≠ |
| Soleado| Templado | Alta | D√©bil | No |
| Soleado| Fr√≠o | Normal | D√©bil | S√≠ |
| Lluvia | Templado | Normal | D√©bil | S√≠ |
| Soleado| Templado | Normal | Fuerte | S√≠ |
| Nublado| Templado | Alta | Fuerte | S√≠ |
| Nublado| Caluroso | Normal | D√©bil | S√≠ |
| Lluvia | Templado | Alta | Fuerte | No |

**Construcci√≥n (Conceptual):**

1.  **Nodo Ra√≠z (14 muestras):** 9 "S√≠" y 5 "No". Es impuro.
2.  **Calcular Ganancia de Informaci√≥n:** El algoritmo calcula la ganancia para "Cielo", "Temperatura", "Humedad" y "Viento".
3.  **Mejor Atributo:** El algoritmo determina que **"Cielo"** es el atributo que mejor divide los datos.
    * *Rama "Soleado" (5 muestras):* 2 "S√≠", 3 "No" -> (Impuro)
    * *Rama "Nublado" (4 muestras):* 4 "S√≠", 0 "No" -> **(Puro -> Hoja: Jugar = S√≠)**
    * *Rama "Lluvia" (5 muestras):* 3 "S√≠", 2 "No" -> (Impuro)
4.  **Recursi√≥n (Rama "Soleado"):**
    * De las 5 muestras de "Soleado", el algoritmo ahora eval√∫a "Temperatura", "Humedad" y "Viento".
    * Descubre que **"Humedad"** es el mejor divisor.
    * *Rama "Humedad = Alta" (3 muestras):* 0 "S√≠", 3 "No" -> **(Puro -> Hoja: Jugar = No)**
    * *Rama "Humedad = Normal" (2 muestras):* 2 "S√≠", 0 "No" -> **(Puro -> Hoja: Jugar = S√≠)**
5.  **Recursi√≥n (Rama "Lluvia"):**
    * De las 5 muestras de "Lluvia", el algoritmo eval√∫a los atributos restantes.
    * Descubre que **"Viento"** es el mejor divisor.
    * *Rama "Viento = Fuerte" (2 muestras):* 0 "S√≠", 2 "No" -> **(Puro -> Hoja: Jugar = No)**
    * *Rama "Viento = D√©bil" (3 muestras):* 3 "S√≠", 0 "No" -> **(Puro -> Hoja: Jugar = S√≠)**

**√Årbol Final Resultante:**

* **¬øCielo?**
    * `Soleado`:
        * **¬øHumedad?**
            * `Alta`: **No Jugar**
            * `Normal`: **S√≠ Jugar**
    * `Nublado`: **S√≠ Jugar**
    * `Lluvia`:
        * **¬øViento?**
            * `Fuerte`: **No Jugar**
            * `D√©bil`: **S√≠ Jugar**

---

## 4. Ejercicio de Construcci√≥n y Soluci√≥n

Construye un √°rbol de decisi√≥n para predecir si un cliente **"Comprar√°" (S√≠/No)** un producto, bas√°ndote en los siguientes datos. Utiliza el criterio de **Ganancia de Informaci√≥n (ID3)**.

**Conjunto de Datos:**

| ID | Edad | Ingreso | Estudiante | ¬øComprar√°? |
| :- | :--- | :--- | :--- | :--- |
| 1 | Joven | Alto | No | No |
| 2 | Joven | Alto | S√≠ | S√≠ |
| 3 | Adulto | Alto | No | S√≠ |
| 4 | Joven | Bajo | No | S√≠ |
| 5 | Adulto | Bajo | S√≠ | S√≠ |
| 6 | Adulto | Bajo | No | S√≠ |
| 7 | Joven | Bajo | S√≠ | S√≠ |

---

### Soluci√≥n del Ejercicio

#### Paso 1: Nodo Ra√≠z

* **Datos Totales (7 muestras):** 6 "S√≠" y 1 "No".
* Necesitamos calcular la Ganancia de Informaci√≥n (Gain) para "Edad", "Ingreso" y "Estudiante" para ver cu√°l es el mejor divisor.

* `Gain(Edad)`:
    * Joven (4 muestras): 3 "S√≠", 1 "No" (Impuro)
    * Adulto (3 muestras): 3 "S√≠", 0 "No" (Puro)
    * El c√°lculo de entrop√≠a muestra una ganancia (Gain) de **0.129**

* `Gain(Ingreso)`:
    * Alto (3 muestras): 2 "S√≠", 1 "No" (Impuro)
    * Bajo (4 muestras): 4 "S√≠", 0 "No" (Puro)
    * El c√°lculo de entrop√≠a muestra una ganancia (Gain) de **0.199**

* `Gain(Estudiante)`:
    * S√≠ (3 muestras): 3 "S√≠", 0 "No" (Puro)
    * No (4 muestras): 3 "S√≠", 1 "No" (Impuro)
    * El c√°lculo de entrop√≠a muestra una ganancia (Gain) de **0.129**

**Decisi√≥n Nivel 1:**
El atributo **"Ingreso"** tiene la Ganancia de Informaci√≥n m√°s alta (0.199). Por lo tanto, es nuestro Nodo Ra√≠z.

* **¬øIngreso?**
    * `Bajo`: (4 muestras: 4 "S√≠", 0 "No") -> **Hoja: Comprar√° = S√≠** (Este nodo es puro, terminamos esta rama).
    * `Alto`: (3 muestras: 2 "S√≠", 1 "No") -> (Este nodo es impuro, debemos seguir dividiendo).

#### Paso 2: Sub-√°rbol (para Ingreso = Alto)

* **Datos restantes (muestras 1, 2, 3):** 2 "S√≠" y 1 "No".
* Atributos restantes: "Edad" y "Estudiante".

* `Gain(Edad)` (para este subconjunto):
    * Joven (Muestras 1, 2): 1 "S√≠", 1 "No" (Impuro)
    * Adulto (Muestra 3): 1 "S√≠", 0 "No" (Puro)
    * Ganancia (Gain): **0.251**

* `Gain(Estudiante)` (para este subconjunto):
    * S√≠ (Muestra 2): 1 "S√≠", 0 "No" (Puro)
    * No (Muestras 1, 3): 1 "S√≠", 1 "No" (Impuro)
    * Ganancia (Gain): **0.251**

**Decisi√≥n Nivel 2:**
Hay un **empate**. Tanto "Edad" como "Estudiante" tienen la misma ganancia. En un empate, el algoritmo ID3 puede elegir arbitrariamente. Elegiremos **"Edad"**.

* **¬øIngreso?**
    * `Bajo`: **S√≠**
    * `Alto`:
        * **¬øEdad?**
            * `Adulto`: (Muestra 3: 1 "S√≠", 0 "No") -> **Hoja: Comprar√° = S√≠** (Puro).
            * `Joven`: (Muestras 1, 2: 1 "S√≠", 1 "No") -> (Impuro, seguir dividiendo).

#### Paso 3: Sub-√°rbol (para Ingreso = Alto Y Edad = Joven)

* **Datos restantes (muestras 1, 2):** 1 "S√≠" y 1 "No".
* Atributo restante: "Estudiante".

* `Gain(Estudiante)`:
    * No (Muestra 1): 0 "S√≠", 1 "No" (Puro)
    * S√≠ (Muestra 2): 1 "S√≠", 0 "No" (Puro)
    * Dividir por "Estudiante" crea dos nodos hoja puros.

**Decisi√≥n Nivel 3:**
Dividimos por **"Estudiante"**.

* `No`: **Hoja: Comprar√° = No**
* `S√≠`: **Hoja: Comprar√° = S√≠**

### √Årbol de Decisi√≥n Final (Soluci√≥n)